{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import math\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Jyp1Kx-ZcvS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass (Assignment 3)"
      ],
      "metadata": {
        "id": "AKNS3hOTYBPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Layer\n",
        "\n",
        "$$ \\mathbf{y} = \\mathbf{x}^T \\mathbf{W} + \\mathbf{b}$$"
      ],
      "metadata": {
        "id": "q7dZfw5pY4de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWXzaeKbWD00"
      },
      "outputs": [],
      "source": [
        "class Linear(object):\n",
        "    \"\"\"\n",
        "    Fully connected layer.\n",
        "\n",
        "    Args:\n",
        "        in_features: number of input features\n",
        "        out_features: number of output features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #      TODO: Define placeholder tensors for layer weight and bias.     #\n",
        "        #       The placeholder tensors should have the correct dimension      #\n",
        "        #        according to the in_features and out_features variables.      #\n",
        "        #                    Note: no for loops are needed!                    #\n",
        "        ########################################################################\n",
        "\n",
        "        self.weight = torch.empty(in_features, out_features)\n",
        "        self.bias = torch.empty(out_features)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self, std=0.1):\n",
        "        \"\"\"\n",
        "        Initialize layer parameters. Sample weight from Gaussian distribution\n",
        "        and bias uniform distribution.\n",
        "\n",
        "        Args:\n",
        "            std: Standard deviation of Gaussian distribution (default: 0.1)\n",
        "        \"\"\"\n",
        "        self.weight = std*torch.randn_like(self.weight)\n",
        "        self.bias = torch.rand_like(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Linear layer: multiply input tensor by weights and add\n",
        "        bias.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        # x: batch * in_features\n",
        "        y = torch.mm(x, self.weight) + self.bias\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU and Sigmoid\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x)=\\max(0,x)\n",
        "$$\n",
        "\n",
        "and the Sigmoid function\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
        "$$*italicized text*"
      ],
      "metadata": {
        "id": "xTOu96L_aHp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(object):\n",
        "    \"\"\"\n",
        "    ReLU non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of ReLU non-linear activation function: y=max(0,x).\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y = torch.clamp(x, min=0)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "class Sigmoid(object):\n",
        "    \"\"\"\n",
        "    Sigmoid non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Sigmoid non-linear activation function: y=1/(1+exp(-x)).\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        y = 1 / (1 + torch.exp(-x))\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "8mkjbIFraKBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE Loss\n",
        "\n",
        "$$\\text{MSE}(\\hat{y},y)= \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}-y)^2 $$"
      ],
      "metadata": {
        "id": "IeWHT0hmbkD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSELoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes mean squared error loss between y_true and y_pred.\n",
        "\n",
        "    Args:\n",
        "      y_true: Tensor containing true labels.\n",
        "      y_pred: Tensor containing predictions.\n",
        "\n",
        "    return:\n",
        "      loss: Mean squared error loss\n",
        "    \"\"\"\n",
        "\n",
        "    ########################################################################\n",
        "    #                  TODO: Implement this function                       #\n",
        "    ########################################################################\n",
        "\n",
        "    loss = ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "    ########################################################################\n",
        "    #                         END OF YOUR CODE                             #\n",
        "    ########################################################################\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "6HSbYxCjboRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation (Assignment 4)"
      ],
      "metadata": {
        "id": "B0G4dAEBcBoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Layer\n",
        "----\n",
        "**NOTE:**\n",
        "\n",
        "For $Z = X Y$:\n",
        "\n",
        "1.   $\\frac{\\partial Z}{\\partial X} = Y^T$\n",
        "2.   $ \\frac{\\partial Z}{\\partial Y} = X^T$\n",
        "\n",
        "Gradient formulas for $y = xW + b$ (given gradient upstreams):\n",
        "\n",
        "1. **Gradient with respect to weights** $ W $:\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial W} = x^T \\cdot \\frac{\\partial L}{\\partial y}\n",
        "   $$\n",
        "\n",
        "2. **Gradient with respect to biases** $ b $:\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{n_{\\text{samples}}} \\frac{\\partial L}{\\partial y_i}\n",
        "   $$\n",
        "   i.e. `dupstream.sum(dim=0)` in codes\n",
        "\n",
        "3. **Gradient with respect to inputs** $ x $ (downstream gradient):\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot W^T\n",
        "   $$"
      ],
      "metadata": {
        "id": "lElo5z70c3VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(object):\n",
        "    \"\"\"\n",
        "    Fully connected layer.\n",
        "\n",
        "    Args:\n",
        "        in_features: number of input features\n",
        "        out_features: number of output features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        ## Y = XW + b\n",
        "        ## X: N * in_features, W: in_features * out_features, XW: N * out_features\n",
        "        self.weight = torch.Tensor(in_features, out_features)\n",
        "        self.bias = torch.Tensor(out_features)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_params()\n",
        "\n",
        "        # NEW: Define a cache variable to save computation, because some of the\n",
        "        # forward pass values would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "        # NEW: Define variables to store the gradients of the weight and bias\n",
        "        # calculated during the backward pass\n",
        "        self.weight_grad = None\n",
        "        self.bias_grad = None\n",
        "\n",
        "    def init_params(self, std=1.):\n",
        "        self.weight = std*torch.randn_like(self.weight)\n",
        "        self.bias = torch.rand_like(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.mm(x, self.weight) + self.bias  # forward pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                TODO: Store input as cache variable                   #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = x\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of linear layer: calculate gradients of loss with respect\n",
        "        to weight and bias and return downstream gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream(dy): Gradient of loss with respect to output of this layer.\n",
        "            **shape**: same as y -> n_samples by out_features\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        ## y = xw + b (CHECK THE NOTE ABOVE!!!!!)\n",
        "        x = self.cache\n",
        "        dy = dupstream\n",
        "\n",
        "        self.weight_grad = torch.mm(x.T, dy)\n",
        "        self.bias_grad = dy.sum(dim=0)\n",
        "        dx = torch.mm(dy, self.weight.T)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ##############w##########################################################\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "AIWk7QZ8de6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU and Sigmoid\n",
        "\n",
        "----\n",
        "**NOTE**:\n",
        "\n",
        "- For $ReLU$:\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial x} = \\text{dupstream} \\odot \\text{ReLU}'(x)\n",
        "  $$\n",
        "\n",
        "- For $Sigmoid$:\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial x} = \\text{dupstream} \\odot \\sigma(x) \\odot (1 - \\sigma(x))\n",
        "  $$\n",
        "\n",
        "$\\odot$: elementwise product\n",
        "\n",
        "----\n",
        "\n",
        "*Extra*:\n",
        "\n",
        "- For $Tanh$:\n",
        "\n",
        "$$\n",
        "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\text{dupstream} \\odot (1 - \\tanh^2(x))\n",
        "$$\n",
        "\n",
        "- For $Leaky\\ ReLU$:\n",
        "\n",
        "$$\n",
        "\\text{Leaky ReLU}(x) = \\begin{cases}\n",
        "    x, & x > 0 \\\\\n",
        "    \\alpha x, & x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x} = \\text{dupstream} \\odot \\begin{cases}\n",
        "    1, & x > 0 \\\\\n",
        "    \\alpha, & x \\leq 0\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "YC1uHjfee2s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(object):\n",
        "    \"\"\"\n",
        "    ReLU non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.clamp(x, min=0)  # forward pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                   TODO: Update cache variable.                       #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = x\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of ReLU non-linear activation function: return downstream\n",
        "        gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "            (=dy): n_samples * n_features\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "            n_samples * n_features\n",
        "        \"\"\"\n",
        "\n",
        "        # Making sure that we don't modify the incoming upstream gradient\n",
        "        dupstream = dupstream.clone()\n",
        "        x = self.cache\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        dupstream[x <= 0] = 0\n",
        "        dx = dupstream\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Sigmoid(object):\n",
        "    \"\"\"\n",
        "    Sigmoid non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__()\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = 1.0 / (1.0 + torch.exp(-x))\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = y\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of Sigmoid non-linear activation function: return\n",
        "        downstream gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        ## !!!ELEMENTWISE PRODUCT: dupstream * σ(x) * (1−σ(x))\n",
        "        y = self.cache\n",
        "        dx = dupstream * y * (1 - y)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "h3JYmosC0HTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE Loss\n",
        "\n",
        "----\n",
        "**NOTE**\n",
        "\n",
        "1. **Gradient w.r.t. each $y_i$** (shape: 1 by n_classes):\n",
        "   $$\n",
        "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{y_i}} = \\frac{2}{n} (\\hat{y}_i - y_i)\n",
        "   $$\n",
        "\n",
        "2. **Gradient w.r.t. $\\hat{y}$** (shape: n_samples by n_classes):\n",
        "   $$\n",
        "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\frac{2}{n} (\\hat{y} - y)\n",
        "   $$\n",
        "   "
      ],
      "metadata": {
        "id": "ItvkCv8X2HAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSELoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes mean squared error loss.\n",
        "\n",
        "    Args:\n",
        "      y_true: Tensor containing true labels.\n",
        "      y_pred: Tensor containing predictions.\n",
        "\n",
        "    Return:\n",
        "      loss: Mean squared error loss.\n",
        "      grad: Gradient of loss w.r.t. y_pred.\n",
        "    \"\"\"\n",
        "    # Calculate mean squared error between y_true and y_pred\n",
        "    loss = torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    ########################################################################\n",
        "    #             TODO: Implement the gradient dL/dy_hat                   #\n",
        "    ########################################################################\n",
        "\n",
        "    ## y: n_samples * n_classes\n",
        "    grad = 2 * (y_pred - y_true) / y_true.numel()  # numel: number of elements\n",
        "\n",
        "    ## !BUG: torch.mean() will change the shape of y\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    return loss, grad"
      ],
      "metadata": {
        "id": "Glfr1oQR2Ktd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax (no backprop)"
      ],
      "metadata": {
        "id": "dV9dmZFa3fM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Softmax(z):\n",
        "    \"\"\"\n",
        "    Computes softmax output for each sample in batch.\n",
        "\n",
        "    Args:\n",
        "      z: Tensor of logits, dimension [batch, classes].\n",
        "\n",
        "    Return:\n",
        "      p: Softmax probability distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################################################################\n",
        "    #                  TODO: Implement Softmax function.                   #\n",
        "    ########################################################################\n",
        "\n",
        "    exp_z = torch.exp(z)\n",
        "    sum_exp = exp_z.sum(dim=1, keepdim=True)  ## keep [batch, 1] but not [batch]\n",
        "    p = exp_z / sum_exp\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "ZXNcsPcU3lBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization (Assignment 5)"
      ],
      "metadata": {
        "id": "C0heT1f45vzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Momentum\n",
        "\n",
        "$$v_i=\\rho v_{i-1}+(1-\\rho)\\nabla_{\\theta}$$\n",
        "\n",
        "$$\\theta^{\\prime}=\\theta-\\epsilon v_i$$"
      ],
      "metadata": {
        "id": "f8lrvWBY50vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum(X, rho, learning_rate, prev_value, index, Grad=Grad_f):\n",
        "    \"\"\"\n",
        "    Gradient descent with momentum optimization step.\n",
        "\n",
        "    Args:\n",
        "        X: Current value of objective function.\n",
        "        rhos: Optimization hyperparameter - see formula above.\n",
        "        learning_rate: Optimization step size.\n",
        "        prev_value: Momentum parameter from previous iteration.\n",
        "        index: Not used.\n",
        "        Grad: Gradient of quadratic function.\n",
        "    \"\"\"\n",
        "    gradient = Grad(*X) # Gradient of current values\n",
        "    v = 0               # Momentum parameter\n",
        "    v_prev = prev_value # Momentum parameter from previous iteration\n",
        "\n",
        "    #############################################################################\n",
        "    #        TODO: Create gradient descent with momentum: update v and X.       #\n",
        "    #############################################################################\n",
        "\n",
        "    v = rho * v_prev + (1 - rho) * gradient\n",
        "    X = X - learning_rate * v\n",
        "\n",
        "    #############################################################################\n",
        "    #                            END OF YOUR CODE                               #\n",
        "    #############################################################################\n",
        "\n",
        "    return X, v"
      ],
      "metadata": {
        "id": "q5URCmF77udE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSProp\n",
        "\n",
        "$$r_i=\\rho r_{i-1}+(1-\\rho)\\nabla^2_{\\theta}$$\n",
        "\n",
        "$$\\theta^{\\prime}=\\theta-\\epsilon \\frac{\\nabla_\\theta}{\\sqrt{r_i+\\delta}}$$"
      ],
      "metadata": {
        "id": "uw130TKQ8LXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSprop(X, rho, learning_rate, prev_value, index, Grad=Grad_f):\n",
        "    \"\"\"\n",
        "    RMSprop optimization step.\n",
        "\n",
        "    Args:\n",
        "        X: Current value of objective function.\n",
        "        rhos: Optimization hyperparameter - see formula above.\n",
        "        learning_rate: Optimization step size.\n",
        "        prev_value: Momentum parameter from previous iteration.\n",
        "        index: Not used.\n",
        "        Grad: Gradient of quadratic function.\n",
        "    \"\"\"\n",
        "    delta = 1e-5        # Tiny amount to prevent division by 0\n",
        "    gradient = Grad(*X) # Gradient of current values\n",
        "    r = 0               # RMSProp parameter\n",
        "    r_prev = prev_value # RMSProp parameter from previous iteration\n",
        "\n",
        "    #############################################################################\n",
        "    #        TODO: Create gradient update with RMSprop: update r and X.         #\n",
        "    #############################################################################\n",
        "\n",
        "    r = rho * r_prev + (1 - rho) * (gradient ** 2)\n",
        "    X = X - learning_rate * gradient / np.sqrt(r + delta)\n",
        "\n",
        "    #############################################################################\n",
        "    #                            END OF YOUR CODE                               #\n",
        "    #############################################################################\n",
        "\n",
        "    return X, r"
      ],
      "metadata": {
        "id": "n09HuhMi8P2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam\n",
        "\n",
        "$$v_i=\\rho_1 v_{i-1}+(1-\\rho_1)\\nabla_{\\theta}$$\n",
        "\n",
        "$$\\hat{v_i}=\\frac{v_i}{1-\\rho^i_1}$$\n",
        "\n",
        "$$r_i=\\rho_2 r_{i-1}+(1-\\rho_2)\\nabla^2_{\\theta}$$\n",
        "\n",
        "$$\\hat{r_i}=\\frac{r_i}{1-\\rho^i_2}$$\n",
        "\n",
        "$$\\theta^{\\prime}=\\theta-\\epsilon \\frac{\\hat{v_i}}{\\sqrt{\\hat{r_i}+\\delta}}$$"
      ],
      "metadata": {
        "id": "jQlUXwUv80nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(X, rhos, learning_rate, prev_values, index, Grad=Grad_f):\n",
        "    \"\"\"\n",
        "    Adam optimization step.\n",
        "\n",
        "    Args:\n",
        "        X: Current value of objective function.\n",
        "        rhos: Optimization hyperparameter - see formula above.\n",
        "        learning_rate: Optimization step size.\n",
        "        prev_value: Momentum parameter from previous iteration.\n",
        "        index: Optimization step counter.\n",
        "        Grad: Gradient of quadratic function.\n",
        "    \"\"\"\n",
        "\n",
        "    delta = 1e-5                 # Tiny amount to prevent division by zero\n",
        "    gradient = Grad(*X)          # Gradient of current values\n",
        "    rho_v, rho_r = rhos          # Rho values for momentum & rmsProp part of Adam\n",
        "    v_prev, r_prev = prev_values # Adam parameters from previous iterations\n",
        "\n",
        "    v = r = 0                    # Adam paramters for momentum & rmsProp\n",
        "    v_bc = r_bc = 0              # Bias corrected adam parameters\n",
        "\n",
        "    #############################################################################\n",
        "    #   TODO: Create gradient update with Adam: update v, v_bc, r, r_bc and X.  #\n",
        "    #############################################################################\n",
        "\n",
        "    v = rho_v * v_prev + (1 - rho_v) * gradient\n",
        "    v_bc = v / (1 - rho_v ** index)\n",
        "\n",
        "    r = rho_r * r_prev + (1 - rho_r) * (gradient ** 2)\n",
        "    r_bc = r / (1 - rho_r ** index)\n",
        "\n",
        "    X = X - learning_rate * v_bc / np.sqrt(r_bc + delta)\n",
        "\n",
        "    #############################################################################\n",
        "    #                            END OF YOUR CODE                               #\n",
        "    #############################################################################\n",
        "\n",
        "    ### ATTENTION!!! return (v, r) but not (v_bc, r_bc) for next iteration\n",
        "    return X, (v, r)"
      ],
      "metadata": {
        "id": "qG-OxZuU84eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Implementations"
      ],
      "metadata": {
        "id": "eclQlfXj9fS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.SGD(net.parameters(), lr=lr, momentum=rho1)  # momentum\n",
        "optimizer2 = optim.RMSprop(net.parameters(), lr=lr, alpha=rho2)  # RMSprop\n",
        "optimizer3 = optim.Adam(net.parameters(), lr=lr, betas=(rho1, rho2))  # Adam"
      ],
      "metadata": {
        "id": "Ali86MRE905r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN (Assignment 8)"
      ],
      "metadata": {
        "id": "nkvAHjL0-I8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Convolutional Layer"
      ],
      "metadata": {
        "id": "iJ0R1Ymu_3tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(object):\n",
        "    \"\"\"\n",
        "    2D convolutional layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        \"\"\"\n",
        "        Initialize the layer with given params\n",
        "\n",
        "        Args:\n",
        "            in_channels: # channels that the input has.\n",
        "            out_channels: # channels that the output will have.\n",
        "            kernel_size: height and width of the kernel in pixels.\n",
        "            stride: # pixels between adjacent receptive fields in both\n",
        "                horizontal and vertical.\n",
        "            padding: # pixels that is used to zero-pad the input.\n",
        "        \"\"\"\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        ########################################################################\n",
        "        #        TODO: Create place holder tensors for weight and bias         #\n",
        "        #                       with correct dimensions.                       #\n",
        "        ########################################################################\n",
        "\n",
        "        \"\"\"\n",
        "        x: input tensor which has a shape of (N, C, H, W)\n",
        "        - N: number of samples\n",
        "        - C: number of channels\n",
        "        - H: height\n",
        "        - W: width\n",
        "\n",
        "        y: output tensor which has a shape of (N, F, H', W')\n",
        "        - F: number of filters (self.out_channels) !!!!!!\n",
        "        \"\"\"\n",
        "        ###### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        num_filters = self.out_channels\n",
        "        self.weight = torch.empty(num_filters, self.in_channels, self.kernel_size, self.kernel_size)\n",
        "        self.bias = torch.empty(num_filters)  # each filter has a bias\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_params()\n",
        "\n",
        "\n",
        "    def init_params(self, std=0.7071):\n",
        "        self.weight = std * torch.randn_like(self.weight)\n",
        "        self.bias = torch.rand_like(self.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of convolutional layer\n",
        "\n",
        "        Args:\n",
        "            x: input tensor which has a shape of (N, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor which has a shape of (N, F, H', W') where\n",
        "                H' = 1 + (H + 2 * padding - kernel_size) / stride\n",
        "                W' = 1 + (W + 2 * padding - kernel_size) / stride\n",
        "        \"\"\"\n",
        "\n",
        "        # Pad the input\n",
        "        x_padded = torch.nn.functional.pad(x, [self.padding] * 4)\n",
        "\n",
        "        # Unpack the needed dimensions\n",
        "        N, _, H, W = x.shape\n",
        "\n",
        "        # Calculate output height and width: h' = 1 + (h + 2p - f) // s\n",
        "        Hp = 1 + (H + 2 * self.padding - self.kernel_size) // self.stride\n",
        "        Wp = 1 + (W + 2 * self.padding - self.kernel_size) // self.stride\n",
        "\n",
        "        # Create an empty output to fill in\n",
        "        y = torch.empty((N, self.out_channels, Hp, Wp), dtype=x.dtype, device=x.device)\n",
        "\n",
        "        ########################################################################\n",
        "        # TODO: Compute the output y by convolving input and weight and add a  #\n",
        "        # bias. Have a good look at our implementation of the backward pass to #\n",
        "        #       get an idea of how to construct the convolution yourself.      #\n",
        "        ########################################################################\n",
        "\n",
        "        for i in range(Hp):\n",
        "            for j in range(Wp):\n",
        "                # Calculate offset for current window on input\n",
        "                h_offset = i * self.stride\n",
        "                w_offset = j * self.stride\n",
        "\n",
        "                window = x_padded[:, :, h_offset:h_offset + self.kernel_size, w_offset:w_offset + self.kernel_size]\n",
        "\n",
        "                # Walk through each sample\n",
        "                for k in range(N):\n",
        "                    # self.weight shape: num_filters, self.in_channels, self.kernel_size, self.kernel_size\n",
        "                    # window shape: num_samples, self.in_channels, self.kernel_size, self.kernel_size\n",
        "                    y[k, :, i, j] = (self.weight * window[k]).sum(dim=(1, 2, 3)) + self.bias\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Cache padded input to use in backward pass\n",
        "        self.cache = x_padded\n",
        "\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "V7XT_toj_8E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Pooling"
      ],
      "metadata": {
        "id": "9yHPQE1oDqoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(object):\n",
        "    \"\"\"\n",
        "    2D max pooling layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride=1, padding=0):\n",
        "        \"\"\"\n",
        "        Initialize the layer with given params\n",
        "\n",
        "        Args:\n",
        "            kernel_size: height and width of the receptive field of the layer\n",
        "                in pixels.\n",
        "            stride: # pixels between adjacent receptive fields in both\n",
        "                horizontal and vertical directions.\n",
        "            padding: # pixels that is used to zero-pad the input.\n",
        "        \"\"\"\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of max pooling layer\n",
        "\n",
        "        Args:\n",
        "            x: input tensor with shape of (N, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor with shape of (N, C, H', W') where\n",
        "                H' = 1 + (H + 2 * padding - kernel_size) / stride\n",
        "                W' = 1 + (W + 2 * padding - kernel_size) / stride\n",
        "        \"\"\"\n",
        "\n",
        "        # Pad the input\n",
        "        x_padded = torch.nn.functional.pad(x, [self.padding] * 4)\n",
        "\n",
        "        # Unpack the needed dimensions\n",
        "        N, C, H, W = x.shape\n",
        "        KS = self.kernel_size\n",
        "\n",
        "        # Calculate output height and width\n",
        "        Hp = 1 + (H + 2 * self.padding - KS) // self.stride\n",
        "        Wp = 1 + (W + 2 * self.padding - KS) // self.stride\n",
        "\n",
        "        # Create an empty output to fill in.\n",
        "        # We combine first and second dim to speed up as we need no loop for each\n",
        "        # channel.\n",
        "        y = torch.empty((N*C, Hp, Wp), dtype=x.dtype, device=x.device)\n",
        "\n",
        "        ########################################################################\n",
        "        #   TODO: Implement forward pass of 2D max pooling layer. It is very   #\n",
        "        #   similar to convolutional layer that you implemented before. Again  #\n",
        "        #                  have a look at the backward pass.                   #\n",
        "        ########################################################################\n",
        "\n",
        "        for i in range(Hp):\n",
        "            for j in range(Wp):\n",
        "                h_offset = i * self.stride\n",
        "                w_offset = j * self.stride\n",
        "\n",
        "                window = x_padded[:, :, h_offset:h_offset + KS, w_offset:w_offset + KS].reshape(N*C, -1)\n",
        "\n",
        "                y[:, i, j], _ = window.max(dim=1)  # return max values and indices\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Reshape output to seperate sample dim from channel dim since we\n",
        "        # combined them\n",
        "        y = y.reshape(N, C, Hp, Wp)\n",
        "\n",
        "        # Cache padded input to use in backward pass\n",
        "        self.cache = x\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "Cf3FQonqD3A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation (Assignment 9)"
      ],
      "metadata": {
        "id": "Dh4HAjhrFqDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 Regularization (Weight Decay)\n",
        "\n",
        "$$\\mathcal{L} = \\mathcal{L}_0 + \\frac{\\lambda}{2}\\sum_w w^2 $$"
      ],
      "metadata": {
        "id": "CYuoXZ66F2lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_wd(train_loader, net, optimizer, criterion, wd):\n",
        "    \"\"\"\n",
        "    Trains network for one epoch in batches.\n",
        "    Uses custom L2 regularization in loss function.\n",
        "\n",
        "    Args:\n",
        "        train_loader: Data loader for training set.\n",
        "        net: Neural network model.\n",
        "        optimizer: Optimizer (e.g. SGD).\n",
        "        criterion: Loss function (e.g. cross-entropy loss).\n",
        "        wd: Weight decay (L2 penalty: lambda)\n",
        "    \"\"\"\n",
        "\n",
        "    avg_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # iterate through batches\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        ########################################################################\n",
        "        #           TODO: Add L2 regularization to loss function.              #\n",
        "        # The total loss becomes: loss = loss + 0.5*wd*l2, where l2 is the sum #\n",
        "        #  of the squared value of all parameters and wd the relative weight   #\n",
        "        #  of the L2 penalty. Hint: loop through the network parameters using  #\n",
        "        #                    'for p in net.parameters():'.                     #\n",
        "        ########################################################################\n",
        "\n",
        "        l2 = 0\n",
        "        for p in net.parameters():\n",
        "            l2 += torch.sum(p * p)\n",
        "        loss += 0.5 * wd * l2\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        # backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep track of loss and accuracy\n",
        "        avg_loss += loss\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return avg_loss/len(train_loader), 100 * correct / total"
      ],
      "metadata": {
        "id": "Ppqkl77QR3wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#            TODO: Add L2 regularization the PyTorch way.              #\n",
        "########################################################################\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=5e-1, weight_decay=3e-3)\n",
        "\n",
        "########################################################################\n",
        "#                          END OF YOUR CODE                            #\n",
        "########################################################################"
      ],
      "metadata": {
        "id": "XrLkefWXUB-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early stopping"
      ],
      "metadata": {
        "id": "brLb-05dUM9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ......\n",
        "\n",
        "# Patience - how many epochs to keep training after accuracy has not improved\n",
        "patience = 5\n",
        "\n",
        "# Initialize early stopping variables\n",
        "val_acc_best = 0\n",
        "patience_cnt = 0\n",
        "\n",
        "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
        "    # Train on data\n",
        "    train_loss, train_acc = train(train_loader,net,optimizer,criterion)\n",
        "\n",
        "    # Test on validation set\n",
        "    val_loss, val_acc = test(val_loader,net,criterion)\n",
        "\n",
        "    writer.add_scalars(\"Loss\", {'Train': train_loss, 'Val': val_loss}, epoch)\n",
        "    writer.add_scalars('Accuracy', {'Train': train_acc, 'Val': val_acc}, epoch)\n",
        "\n",
        "    ########################################################################\n",
        "    #                   TODO: Implement early stopping.                    #\n",
        "    #         Hint: use 'break' to break out of the training loop.         #\n",
        "    ########################################################################\n",
        "\n",
        "    if val_acc > val_acc_best:\n",
        "        val_acc_best = val_acc\n",
        "        patience_cnt = 0\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= patience:\n",
        "            break\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################"
      ],
      "metadata": {
        "id": "D0MhJbVAUQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ],
      "metadata": {
        "id": "rbQ0vSUhU8ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNet_do(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple fully connected neural network with residual connections and dropout\n",
        "    layers in PyTorch. Layers are defined in __init__ and forward pass\n",
        "    implemented in forward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FCNet_do, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(40, 500)\n",
        "        self.fc2 = nn.Linear(500, 500)\n",
        "        self.fc3 = nn.Linear(500, 500)\n",
        "        self.fc4 = nn.Linear(500, 500)\n",
        "        self.fc5 = nn.Linear(500, 500)\n",
        "        self.fc6 = nn.Linear(500, 10)\n",
        "\n",
        "        ########################################################################\n",
        "        #                     TODO: Define dropout layers.                     #\n",
        "        ########################################################################\n",
        "\n",
        "        # p: probability of an element to be zeroed\n",
        "        p = 0.4\n",
        "        self.do1 = nn.Dropout(p=p)\n",
        "        self.do2 = nn.Dropout(p=p)\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################################################################\n",
        "        #         TODO: Modify forward pass to include dropout layers.         #\n",
        "        ########################################################################\n",
        "\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = h + F.relu(self.fc2(h))\n",
        "        h = self.do1(h)\n",
        "        h = h + F.relu(self.fc3(h))\n",
        "        h = h + F.relu(self.fc4(h))\n",
        "        h = self.do2(h)\n",
        "        h = h + F.relu(self.fc5(h))\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        return self.fc6(h)"
      ],
      "metadata": {
        "id": "qaV6ti2NVc7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN (Assignment 10)"
      ],
      "metadata": {
        "id": "SXWGAySaV92c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla RNN"
      ],
      "metadata": {
        "id": "-RHERSlnWOaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Vanilla recurrent neural network (a.k.a. Elman RNN) which has the following\n",
        "    update rule:\n",
        "        ht​ = tanh(​xt * W_xh  ​+ b_xh ​+ ​h(t−1) * W_hh ​+ b_hh​)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input to hidden weights\n",
        "        self.weight_xh = None\n",
        "\n",
        "        # Hidden to hidden weights\n",
        "        self.weight_hh = None\n",
        "\n",
        "        # Input to hidden biases\n",
        "        self.bias_xh = None\n",
        "\n",
        "        # Hidden to hidden biases\n",
        "        self.bias_hh = None\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Create weight and bias tensors with given name above with   #\n",
        "        #                             correct sizes.                           #\n",
        "        # NOTE: Don't forget to encapsulate weights and biases in nn.Parameter #\n",
        "        ########################################################################\n",
        "\n",
        "        ## for weight_xh: treated as Linear Layer\n",
        "        self.weight_xh = nn.Parameter(torch.empty(input_size, hidden_size))\n",
        "        self.weight_hh = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
        "        self.bias_xh = nn.Parameter(torch.empty(hidden_size))\n",
        "        self.bias_hh = nn.Parameter(torch.empty(hidden_size))\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.reset_params()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input with shape (N, T, D) where N is number of samples, T is\n",
        "                number of timestep and D is input size which must be equal to\n",
        "                self.input_size.\n",
        "\n",
        "        Returns:\n",
        "            y: output with a shape of (N, T, H) where H is hidden size\n",
        "        \"\"\"\n",
        "\n",
        "        # Transpose input for efficient vectorized calculation. After transposing\n",
        "        # the input will have (T, N, D).\n",
        "        x = x.transpose(0, 1)  ## swap dim 0 and dim 1\n",
        "\n",
        "        # Unpack dimensions\n",
        "        T, N = x.shape[0], x.shape[1]\n",
        "\n",
        "        # Initialize hidden states to zero. There will be one hidden state for\n",
        "        # each input, so it will have shape of (N, H)\n",
        "        h0 = torch.zeros(N, self.hidden_size, device=x.device)\n",
        "\n",
        "        # Define a list to store outputs. We will then stack them.\n",
        "        y = []\n",
        "\n",
        "        ########################################################################\n",
        "        #             TODO: Implement forward pass of Vanilla RNN              #\n",
        "        ########################################################################\n",
        "\n",
        "        ht_1 = h0  # h(t-1)\n",
        "        for t in range(T):\n",
        "            # torch.addmm(b, mat1, mat2) <=> b + torch.mm(mat1, mat2)\n",
        "            xh = torch.addmm(self.bias_xh, x[t], self.weight_xh)\n",
        "            hh = torch.addmm(self.bias_hh, ht_1, self.weight_hh)\n",
        "            ht = torch.tanh(xh + hh)\n",
        "\n",
        "            y.append(ht)\n",
        "            ht_1 = ht\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Stack the outputs. After this operation, output will have shape of\n",
        "        # (T, N, H)\n",
        "        y = torch.stack(y)\n",
        "\n",
        "        # Switch time and batch dimension, (T, N, H) -> (N, T, H)\n",
        "        y = y.transpose(0, 1)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "61CoptYsWOVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "\n",
        "$$\n",
        "\\mathbf{r}^{(t)} = \\sigma \\left( W_{hr} \\mathbf{h}^{(t-1)} + \\mathbf{b}_{hr} + W_{xr} \\mathbf{x}^{(t)} + \\mathbf{b}_{xr}\\right).\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{z}^{(t)} = \\sigma \\left( W_{hz} \\mathbf{h}^{(t-1)} + \\mathbf{b}_{hz} + W_{xz} \\mathbf{x}^{(t)} + \\mathbf{b}_{xz}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{n}^{(t)} = \\tanh \\left( \\mathbf{r}^{(t)} \\odot \\left( W_{hn} \\mathbf{h}^{(t-1)} + \\mathbf{b}_{hn} \\right) + W_{xn} \\mathbf{x}^{(t)} + \\mathbf{b}_{xn}\\right).\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{h}^{(t)} = \\underbrace{\\mathbf{z}^{(t)} \\odot \\mathbf{h}^{(t-1)}}_{\\text{resets or puts through}} + \\underbrace{\\left( \\mathbf{1}- \\mathbf{z}^{(t)} \\right) \\odot \\mathbf{n}^{(t)}}_{\\text{adds new information (or not)}}.\n",
        "$$"
      ],
      "metadata": {
        "id": "_LApfuTmYDzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated recurrent unit which has the following update rule:\n",
        "        rt​ = σ(W_xr * ​xt​ + b_xr​ + W_hr * ​h(t−1) ​+ b_hr​)\n",
        "        zt​ = σ(W_xz * ​xt​ + b_xz​ + W_hz * ​h(t−1) ​+ b_hz​)\n",
        "        nt​ = tanh(W_xn * ​xt ​+ b_xn ​+ rt​ ⊙ ​(W_hn * ​h(t−1) ​+ b_hn​))\n",
        "        ht​ = (1 − zt​) ⊙ nt ​+ zt​ ⊙ h(t−1)​​\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input to hidden weights\n",
        "        self.weight_xh = None\n",
        "\n",
        "        # Hidden to hidden weights\n",
        "        self.weight_hh = None\n",
        "\n",
        "        # Input to hidden biases\n",
        "        self.bias_xh = None\n",
        "\n",
        "        # Hidden to hidden biases\n",
        "        self.bias_hh = None\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Create weight and bias tensors with given name above with   #\n",
        "        #                             correct sizes.                           #\n",
        "        # NOTE: Don't forget to encapsulate weights and biases in nn.Parameter #\n",
        "        # NOTE: Make sure that weights and biases are concatanations: that is, #\n",
        "        #       self.weight_xh = [W_xr; W_xz; W_xn]. Do a similar thing for    #\n",
        "        #       self.weight_hh, self.bias_xh, and self.bias_hh.\n",
        "        ########################################################################\n",
        "\n",
        "        # W_xh = [W_xr, W_xz, W_xn]\n",
        "        self.weight_xh = nn.Parameter(torch.empty(input_size, 3 * hidden_size))\n",
        "\n",
        "        # W_hh = [W_hr, W_hz, W_hn]\n",
        "        self.weight_hh = nn.Parameter(torch.empty(hidden_size, 3 * hidden_size))\n",
        "\n",
        "        # b_xh = [b_xr, b_xz, b_xn]\n",
        "        self.bias_xh = nn.Parameter(torch.empty(3 * hidden_size))\n",
        "\n",
        "        # b_hh = [b_hr, b_hz, b_hn]\n",
        "        self.bias_hh = nn.Parameter(torch.empty(3 * hidden_size))\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.reset_params()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input with shape (N, T, D) where N is number of samples, T is\n",
        "                number of timestep and D is input size which must be equal to\n",
        "                self.input_size.\n",
        "\n",
        "        Returns:\n",
        "            y: output with a shape of (N, T, H) where H is hidden size\n",
        "        \"\"\"\n",
        "\n",
        "        # Transpose input for efficient vectorized calculation. After transposing\n",
        "        # the input will have (T, N, D).\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Unpack dimensions\n",
        "        T, N, H = x.shape[0], x.shape[1], self.hidden_size\n",
        "\n",
        "        # Initialize hidden states to zero. There will be one hidden state for\n",
        "        # each input, so it will have shape of (N, H)\n",
        "        h0 = torch.zeros(N, H, device=x.device)\n",
        "\n",
        "        # Define a list to store outputs. We will then stack them.\n",
        "        y = []\n",
        "\n",
        "        ########################################################################\n",
        "        #                 TODO: Implement forward pass of GRU                  #\n",
        "        ########################################################################\n",
        "\n",
        "        ht_1 = h0\n",
        "        for t in range(T):\n",
        "            # xh: [b_xr, b_xz, b_xn] + x[t] * [W_xr, W_xz, W_xn]\n",
        "            #   = [b_xr + x[t] * W_xr, b_xz + x[t] * W_xz, b_xn + x[t] * W_xn]\n",
        "            # shape: (N, 3*H)\n",
        "            xh = torch.addmm(self.bias_xh, x[t], self.weight_xh)\n",
        "\n",
        "            # hh: [b_hr, b_hz, b_hn] + ht_1 * [W_hr, W_hz, W_hn]\n",
        "            #   = [b_hr + ht_1 * W_hr, b_hz + ht_1 * W_hz, b_hn + ht_1 * W_hn]\n",
        "            hh = torch.addmm(self.bias_hh, ht_1, self.weight_hh)\n",
        "\n",
        "            # rt​ = σ(W_xr * ​xt​ + b_xr​ + W_hr * ​h(t−1) ​+ b_hr​)\n",
        "            rt = torch.sigmoid(xh[:, :H] + hh[:, :H])\n",
        "            # zt​ = σ(W_xz * ​xt​ + b_xz​ + W_hz * ​h(t−1) ​+ b_hz​)\n",
        "            zt = torch.sigmoid(xh[:, H:2 * H] + hh[:, H:2 * H])\n",
        "\n",
        "            # nt​ = tanh(W_xn * ​xt ​+ b_xn ​+ rt​ ⊙ ​(W_hn * ​h(t−1) ​+ b_hn​))\n",
        "            nt = torch.tanh(xh[:, 2 * H:] + rt * hh[:, 2 * H:])\n",
        "\n",
        "            # ht​ = (1 − zt​) ⊙ nt ​+ zt​ ⊙ h(t−1)​​\n",
        "            ht = (1 - zt) * nt + zt * ht_1\n",
        "\n",
        "            y.append(ht)\n",
        "            ht_1 = ht\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Stack the outputs. After this operation, output will have shape of\n",
        "        # (T, N, H)\n",
        "        y = torch.stack(y)\n",
        "\n",
        "        # Switch time and batch dimension, (T, N, H) -> (N, T, H)\n",
        "        y = y.transpose(0, 1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "xIYHnR_5YXFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n",
        "\n",
        "Input gate:\n",
        "$$\n",
        "I_t = \\sigma(X_t W_{xi} + H_{t-1} W_{hi} + b_i)\n",
        "$$\n",
        "\n",
        "Forget gate:\n",
        "$$\n",
        "F_t = \\sigma(X_t W_{xf} + H_{t-1} W_{hf} + b_f)\n",
        "$$\n",
        "\n",
        "Output gate:\n",
        "$$\n",
        "O_t = \\sigma(X_t W_{xo} + H_{t-1} W_{ho} + b_o)\n",
        "$$\n",
        "\n",
        "Candidate memory cell:\n",
        "$$\n",
        "\\tilde{C}_t = \\tanh(X_t W_{xc} + H_{t-1} W_{hc} + b_c)\n",
        "$$\n",
        "\n",
        "LSTM cell state update:\n",
        "$$\n",
        "C_t = F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t\n",
        "$$\n",
        "\n",
        "Current hidden state:\n",
        "$$\n",
        "H_t = O_t \\odot \\tanh(C_t)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "wGRe8dn8Z-qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory recurrent unit which has the following update rule:\n",
        "        it ​= σ(W_xi * ​xt ​+ b_xi ​+ W_hi * ​h(t−1) ​+ b_hi​)\n",
        "        ft​ = σ(W_xf * ​xt ​+ b_xf ​+ W_hf * ​h(t−1) ​+ b_hf​)\n",
        "        gt ​= tanh(W_xg * ​xt ​+ b_xg ​+ W_hg * ​h(t−1) ​+ b_hg​)\n",
        "        ot ​= σ(W_xo * ​xt ​+ b_xo​ + W_ho ​h(t−1) ​+ b_ho​)\n",
        "        ct ​= ft​ ⊙ c(t−1) ​+ it ​⊙ gt​\n",
        "        ht ​= ot​ ⊙ tanh(ct​)​\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input to hidden weights\n",
        "        self.weight_xh = None\n",
        "\n",
        "        # Hidden to hidden biases\n",
        "        self.weight_hh = None\n",
        "\n",
        "        # Input to hidden biases\n",
        "        self.bias_xh = None\n",
        "\n",
        "        # Hidden to hidden biases\n",
        "        self.bias_hh = None\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Create weight and bias tensors with given name above with   #\n",
        "        #                             correct sizes.                           #\n",
        "        # NOTE: Don't forget to encapsulate weights and biases in nn.Parameter #\n",
        "        ########################################################################\n",
        "\n",
        "        # W_xh = [W_xi, W_xf, W_xg, W_xo]\n",
        "        self.weight_xh = nn.Parameter(torch.empty(input_size, 4 * hidden_size))\n",
        "        self.weight_hh = nn.Parameter(torch.empty(hidden_size, 4 * hidden_size))\n",
        "        self.bias_xh = nn.Parameter(torch.empty(4 * hidden_size))\n",
        "        self.bias_hh = nn.Parameter(torch.empty(4 * hidden_size))\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.reset_params()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input with shape (N, T, D) where N is number of samples, T is\n",
        "                number of timestep and D is input size which must be equal to\n",
        "                self.input_size.\n",
        "\n",
        "        Returns:\n",
        "            y: output with a shape of (N, T, H) where H is hidden size\n",
        "        \"\"\"\n",
        "\n",
        "        # Transpose input for efficient vectorized calculation. After transposing\n",
        "        # the input will have (T, N, D).\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Unpack dimensions\n",
        "        T, N, H = x.shape[0], x.shape[1], self.hidden_size\n",
        "\n",
        "        # Initialize hidden and cell states to zero. There will be one hidden\n",
        "        # and cell state for each input, so they will have shape of (N, H)\n",
        "        h0 = torch.zeros(N, H, device=x.device)\n",
        "        c0 = torch.zeros(N, H, device=x.device)\n",
        "\n",
        "        # Define a list to store outputs. We will then stack them.\n",
        "        y = []\n",
        "\n",
        "        ########################################################################\n",
        "        #                 TODO: Implement forward pass of LSTM                 #\n",
        "        ########################################################################\n",
        "\n",
        "        ht_1 = h0\n",
        "        ct_1 = c0\n",
        "        for t in range(T):\n",
        "            xh = torch.addmm(self.bias_xh, x[t], self.weight_xh)\n",
        "            hh = torch.addmm(self.bias_hh, ht_1, self.weight_hh)\n",
        "\n",
        "            it = torch.sigmoid(xh[:, 0:H] + hh[:, 0:H])\n",
        "            ft = torch.sigmoid(xh[:, H:2 * H] + hh[:, H:2 * H])\n",
        "            gt = torch.tanh(xh[:, 2 * H: 3 * H] + hh[:, 2 * H:3 * H])\n",
        "            ot = torch.sigmoid(xh[:, 3 * H: 4 * H] + hh[:, 3 * H:4 * H])\n",
        "\n",
        "            ct = it * gt + ft * ct_1\n",
        "            ht = ot * torch.tanh(ct)\n",
        "\n",
        "            y.append(ht)\n",
        "            ct_1 = ct\n",
        "            ht_1 = ht\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Stack the outputs. After this operation, output will have shape of\n",
        "        # (T, N, H)\n",
        "        y = torch.stack(y)\n",
        "\n",
        "        # Switch time and batch dimension, (T, N, H) -> (N, T, H)\n",
        "        y = y.transpose(0, 1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "0zGWySHwbIf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention"
      ],
      "metadata": {
        "id": "FVESYgwocDYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Self Attention\n",
        "\n",
        "$$\\qquad w_{ij}' = \\mathbf{x_i}^T\\mathbf{x_j}$$\n",
        "\n",
        "$$w_{ij}  = \\frac{\\exp w_{ij}'}{\\sum_j \\exp w_{ij}'}$$\n",
        "\n",
        "$$\\mathbf{y_i} = \\sum_j w_{ij}\\mathbf{x_j},$$\n"
      ],
      "metadata": {
        "id": "xzQdsIGnc9NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic self-attention operation.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################################################################\n",
        "        #      TODO: Perform the basic self-attention operation. Calculate     #\n",
        "        #        w_prime, apply softmax and compute the output tensor y.       #\n",
        "        ########################################################################\n",
        "\n",
        "        # x: [b, t, k], batch size, sequence length, embedding dimension\n",
        "\n",
        "        # w: [b, t, t]\n",
        "        w_prime = torch.bmm(x, x.transpose(1, 2))\n",
        "        w = F.softmax(w_prime, dim=2)\n",
        "\n",
        "        # y: same shape as x [b, t, k]\n",
        "        y = torch.bmm(w, x)\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "XmgkcD3mdJ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Queries, keys and values\n",
        "\n",
        "$$\\mathbf{q_i} = \\mathbf{W_q} \\mathbf{x_i} \\qquad \\mathbf{k_i} = \\mathbf{W_k} \\mathbf{x_i} \\qquad \\mathbf{v_i} = \\mathbf{W_v} \\mathbf{x_i}$$\n",
        "\n",
        "$$w_{ij}' = \\frac{\\mathbf{q_i}^T \\mathbf{k_j}}{\\sqrt{k}}$$\n",
        "\n",
        "$$\\qquad w_{ij} = \\text{softmax}(w_{ij}')$$\n",
        "\n",
        "$$\\qquad \\mathbf{y_i} = \\sum_j w_{ij}\\mathbf{v_j}$$"
      ],
      "metadata": {
        "id": "OUj3E89AdWq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-attention operation with learnable key, query and value embeddings.\n",
        "\n",
        "    Args:\n",
        "        k: embedding dimension\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, k):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        # These compute the queries, keys and values\n",
        "        self.tokeys    = nn.Linear(k, k, bias=False)\n",
        "        self.toqueries = nn.Linear(k, k, bias=False)\n",
        "        self.tovalues  = nn.Linear(k, k, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get tensor dimensions: batch size, sequence length and embedding dimension\n",
        "        b, t, k = x.size()\n",
        "\n",
        "        ########################################################################\n",
        "        #   TODO: Perform self-attention operation with learnable query, key   #\n",
        "        #         and value mappings. Calculate w_prime, apply scaling,        #\n",
        "        #               softmax and compute the output tensor y.               #\n",
        "        ########################################################################\n",
        "\n",
        "        # q, k, v: shape [b, t, k]\n",
        "        query = self.toqueries(x)\n",
        "        key = self.tokeys(x)\n",
        "        value = self.tovalues(x)\n",
        "\n",
        "        w_prime = torch.bmm(query, key.transpose(1, 2))\n",
        "\n",
        "        w_prime = w_prime / (k ** 0.5)\n",
        "\n",
        "        w = F.softmax(w_prime, dim=2)\n",
        "        y = torch.bmm(w, value)\n",
        "\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "4GzHang-drRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention"
      ],
      "metadata": {
        "id": "PZBLAQ-4eInG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Wide mult-head self-attention layer.\n",
        "\n",
        "    Args:\n",
        "        k: embedding dimension\n",
        "        heads: number of heads (k mod heads must be 0)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, k, heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.heads = heads\n",
        "\n",
        "        # These compute the queries, keys and values for all\n",
        "        # heads (as a single concatenated vector)\n",
        "        self.tokeys    = nn.Linear(k, k * heads, bias=False)\n",
        "        self.toqueries = nn.Linear(k, k * heads, bias=False)\n",
        "        self.tovalues  = nn.Linear(k, k * heads, bias=False)\n",
        "\n",
        "        # This unifies the outputs of the different heads into\n",
        "        # a single k-vector\n",
        "        self.unifyheads = nn.Linear(k * heads, k)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, t, k = x.size()\n",
        "        h = self.heads\n",
        "\n",
        "        ########################################################################\n",
        "        #     TODO: Perform wide multi-head self-attention operation with      #\n",
        "        #   learnable query, key and value mappings. Calculate w_prime, apply  #\n",
        "        #  scaling, softmax and compute and concatenate the output tensors y,  #\n",
        "        #        and transform back to the original embedding dimension.       #\n",
        "        ########################################################################\n",
        "\n",
        "        # [b, t, k * h] => [b, t, h, k]\n",
        "        keys = self.tokeys(x).view(b, t, h, k)\n",
        "        queries = self.toqueries(x).view(b, t, h, k)\n",
        "        values = self.tovalues(x).view(b, t, h, k)\n",
        "\n",
        "        # [b, t, h, k] => [b, h, t, k] => [b * h, t, k]\n",
        "        keys = keys.transpose(1, 2).reshape(b * h, t, k)\n",
        "        queries = queries.transpose(1, 2).reshape(b * h, t, k)\n",
        "        values = values.transpose(1, 2).reshape(b * h, t, k)\n",
        "\n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2))  # [b * h, t, t]\n",
        "\n",
        "        w_prime = w_prime / (k ** 0.5)\n",
        "        w = F.softmax(w_prime, dim=2)\n",
        "        y = torch.bmm(w, values)  # [b * h, t, k]\n",
        "\n",
        "        # [b * h, t, k] => [b, h, t, k] => [b, t, h, k] => [b, t, h * k]\n",
        "        y = y.reshape(b, h, t, k).transpose(2, 1).reshape(b, t, h * k)\n",
        "\n",
        "        y = self.unifyheads(y)  # [b, t, k]\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "O026XlYHeO04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block"
      ],
      "metadata": {
        "id": "njxXGNFgesVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![transformer.PNG](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "CQ1wHGPVfBmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, k, heads):\n",
        "        \"\"\"\n",
        "        Basic transformer block.\n",
        "\n",
        "        Args:\n",
        "            k: embedding dimension\n",
        "            heads: number of heads (k mod heads must be 0)\n",
        "\n",
        "        \"\"\"\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.att = MultiHeadAttention(k, heads=heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(k)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(k, 4 * k),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * k, k))\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: input with shape of (b, t, k)\n",
        "\n",
        "        Returns:\n",
        "            y: output with shape of (b, t, k)\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #        TODO: Perform the forward pass of a transformer block         #\n",
        "        #                       as depicted in the image.                      #\n",
        "        ########################################################################\n",
        "\n",
        "        y = x + self.att(x)\n",
        "\n",
        "        y = self.norm1(y)\n",
        "\n",
        "        y = y + self.ff(y)\n",
        "\n",
        "        y = self.norm2(y)\n",
        "\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "T1WdXCasewjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning"
      ],
      "metadata": {
        "id": "W6RMyz73ftUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoEncoder"
      ],
      "metadata": {
        "id": "k4yd-a32fxKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Create the necessary layers                                 #\n",
        "        ########################################################################\n",
        "\n",
        "        \"\"\"\n",
        "        latent_dims: latent space dimension (int)\n",
        "        s_img: size of square input image (int)\n",
        "        hdim: dimensions of hidden layers (list)\n",
        "        \"\"\"\n",
        "        self.linear1 = nn.Linear(s_img * s_img, hdim[0])\n",
        "        self.linear2 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3 = nn.Linear(hdim[1], latent_dims)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Apply final layer of the enconder                           #\n",
        "        #    NOTE: must you apply an activation, as before?                    #\n",
        "        ########################################################################\n",
        "\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return x\n",
        "\n",
        "#decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(latent_dims, hdim[1])\n",
        "        self.linear2 = nn.Linear(hdim[1], hdim[0])\n",
        "        self.linear3 = nn.Linear(hdim[0], s_img*s_img)\n",
        "        self.relu    = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Apply full forward function                                 #\n",
        "        #    NOTE: Please have a close look at the forward function of the     #\n",
        "        #    encoder                                                           #\n",
        "        ########################################################################\n",
        "\n",
        "        z = self.relu(self.linear1(z))\n",
        "        z = self.relu(self.linear2(z))\n",
        "        z = self.sigmoid(self.linear3(z))\n",
        "\n",
        "        z = z.reshape(-1, 1, s_img, s_img)  # image channels = 1\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return z\n",
        "\n",
        "#autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(latent_dims, s_img, hdim)\n",
        "        self.decoder = Decoder(latent_dims, s_img, hdim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: concatanate encoder and decoder                             #\n",
        "        ########################################################################\n",
        "\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "LkPGq2ewf0ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Auto Encoders (VAE)"
      ],
      "metadata": {
        "id": "5vJH9FLHgaga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder\n",
        "class VarEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim):\n",
        "        super(VarEncoder, self).__init__()\n",
        "\n",
        "        #layers for g1\n",
        "        self.linear1_1 = nn.Linear(s_img*s_img, hdim[0])\n",
        "        self.linear2_1 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3_1 = nn.Linear(hdim[1], latent_dims)\n",
        "\n",
        "        #layers for g2\n",
        "        self.linear1_2 = nn.Linear(s_img*s_img, hdim[0])\n",
        "        self.linear2_2 = nn.Linear(hdim[0], hdim[1])\n",
        "        self.linear3_2 = nn.Linear(hdim[1], latent_dims)\n",
        "\n",
        "        self.relu    = nn.ReLU()\n",
        "\n",
        "        #distribution setup\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.to(try_gpu()) # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale.to(try_gpu())\n",
        "        self.kl = 0\n",
        "\n",
        "    ########################################################################\n",
        "    #    TODO: Define function for:                                        #\n",
        "    #    1. the Kullback-Leibner loss \"kull_leib\"                          #\n",
        "    #    2. the Reparameterization trick                                   #\n",
        "    ########################################################################\n",
        "\n",
        "    def kull_leib(self, mu, sig):\n",
        "        return (sig ** 2 + mu ** 2 - torch.log(sig) - 1/2).sum()\n",
        "\n",
        "    def reparameterize(self, mu, sig):\n",
        "        return mu + sig * self.N.sample(mu.shape)\n",
        "\n",
        "    ########################################################################\n",
        "    #                         END OF YOUR CODE                             #\n",
        "    ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #    TODO: Compute mean and variance                                   #\n",
        "        ########################################################################\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        x1 = self.relu(self.linear1_1(x))\n",
        "        x1 = self.relu(self.linear2_1(x1))\n",
        "        x1 = self.linear3_1(x1)\n",
        "\n",
        "        x2 = self.relu(self.linear1_2(x))\n",
        "        x2 = self.relu(self.linear2_2(x2))\n",
        "        x2 = self.linear3_2(x2)\n",
        "\n",
        "        sig = torch.exp(x1)  ## !!!!!\n",
        "        mu = x2\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        #reparameterize to find z\n",
        "        z = self.reparameterize(mu, sig)\n",
        "\n",
        "        #loss between N(0,I) and learned distribution\n",
        "        self.kl = self.kull_leib(mu, sig)\n",
        "\n",
        "        return z\n",
        "\n",
        "#decoder: same as before\n",
        "\n",
        "#autoencoder\n",
        "class VarAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims, s_img, hdim = [100, 50]):\n",
        "        super(VarAutoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = VarEncoder(latent_dims, s_img, hdim)\n",
        "        self.decoder = Decoder(latent_dims, s_img, hdim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        z = self.encoder(x)\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "uo-wWcZWgfFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}